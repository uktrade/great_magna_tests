version: 2

# Variables
load_hatch_rate: &load_hatch_rate
  HATCH_RATE: 3

load_num_users: &load_num_users
  NUM_USERS: 1000

load_run_time: &load_run_time
  RUN_TIME: 5m

# Machines configs
machine_python3_clean: &machine_python3_clean
  docker:
    - image: circleci/python:3.8.0

machine_python3_node: &machine_python3_node
  docker:
    - image: circleci/python:3.8.0-node

machine_python3_node_browsers: &machine_python3_node_browsers
  docker:
    - image: circleci/python:3.8.0-node-browsers

# Individual (shared) steps.
step_checkout_repo: &step_checkout_repo
                      checkout

step_install_requirements_for_parallel_browser_tests: &step_install_requirements_for_parallel_browser_tests
  run:
    name: Create virtualenv and install dependencies
    when: always
    command: |
      python3 -m venv venv
      . venv/bin/activate
      pip install --quiet --upgrade pip
      pip install --quiet -r requirements_browser.txt

step_install_allure_cli: &step_install_allure_cli
  run:
    name: Install Allure command line tool
    when: always
    command: |
      wget https://github.com/allure-framework/allure2/releases/download/2.13.1/allure-commandline-2.13.1.zip -O allure.zip
      unzip -q allure.zip
      find . -maxdepth 1 -type d -name "allure*" -exec mv {} allure \;

step_install_moxci: &step_install_moxci
  run:
    name: Install moxci to send CircleCI notifacations to Slack
    when: always
    command: |
      sudo npm install -g moxci@0.1.3

step_save_test_related_env_vars: &step_save_test_related_env_vars
  run:
    name: Saving environment variables that control test execution
    when: always
    command: |
      . venv/bin/activate
      echo "Saving environment variables that control test execution"
      ./save_test_related_env_vars.py

step_make_allure_report: &step_make_allure_report
  run:
    name: Generate Allure report
    when: always
    command: |
      export JAVA_HOME=$(dirname $(dirname $(readlink -f $(which javac))))
      export PATH=$PATH:$HOME/bin:$PATH:$JAVA_HOME/bin:./allure/bin
      make report

step_update_allure_result_files_generated_by_browser_tests: &step_update_allure_result_files_generated_by_browser_tests
  run:
    name: Update Allure result files generated by Browser tests
    when: always
    command: |
      set -e
      mv tests/browser/results* .
      ls -lad results*
      make results_browser

step_install_junit_merge: &step_install_junit_merge
  run:
    name: Install an npm package to merge JUnit XML report files
    when: always
    command: |
      sudo npm install -g junit-merge@2.0.0

step_install_requirements_for_periodic_tasks: &step_install_requirements_for_periodic_tasks
  run:
    name: Create virtualenv and install dependencies
    command: |
      python3 -m venv venv
      . venv/bin/activate
      pip install --upgrade pip
      pip install -q -r requirements_periodic_tasks.txt

step_cache_periodic_tasks_requirements: &step_cache_periodic_tasks_requirements
  save_cache:
    key: periodic-tasks-dependency-cache-{{ .Revision }}
    paths:
      - ~/great_magna_tests

step_prepare_list_of_scenarios_to_run_by_parallel_browser_tests: &step_prepare_list_of_scenarios_to_run_by_parallel_browser_tests
  run:
    name: Prepare a list of scenarios to run
    command: |
      . venv/bin/activate
      python ./env_vars/env_writer.py --env=${TEST_ENV} --config=./env_vars/env.json
      source ./env_vars/.env_with_export
      export PATH=$PATH:$HOME/bin
      if [ 0 = 0 ];
      then
          export BROWSER="chrome";
      else
          export BROWSER="firefox";
      fi
      cd tests/browser
#      PYTHONPATH=. behave \
#        --dry-run \
#        --no-source \
#        --no-summary \
#        --no-snippets \
#        --no-skipped \
#        --tags=~@wip \
#        --tags=~@fixme \
#        --tags=~@skip \
#        --tags=~@eu-exit \
#        ${TAGS} \
#        --format=mini \
#        features/ \
#        | grep -v "Supplied path\|Trying base " > scenario_titles.txt

step_install_requirements_for_load_tests: &step_install_requirements_for_load_tests
  run:
    name: Create virtualenv and install dependencies
    command: |
      python3 -m venv venv
      . venv/bin/activate
      pip install --quiet --upgrade pip
      pip install --quiet -r requirements_load.txt



step_cache_browser_tests_requirements: &step_cache_browser_tests_requirements
  save_cache:
    key: browser-tests-dependency-cache-{{ .Environment.CIRCLE_WORKFLOW_ID }}
    paths:
      - ~/great_magna_tests

step_cache_load_tests_requirements: &step_cache_load_tests_requirements
  save_cache:
    key: load-tests-dependency-cache-{{ .Environment.CIRCLE_WORKFLOW_ID }}
    paths:
      - ~/great_magna_tests

step_restore_cached_browser_tests_requirements: &step_restore_cached_browser_tests_requirements
  restore_cache:
    keys:
      - browser-tests-dependency-cache-{{ .Environment.CIRCLE_WORKFLOW_ID }}

step_restore_cached_load_tests_requirements: &step_restore_cached_load_tests_requirements
  restore_cache:
    keys:
      - load-tests-dependency-cache-{{ .Environment.CIRCLE_WORKFLOW_ID }}

step_run_browser_tests_in_parallel: &step_run_browser_tests_in_parallel
  run:
    name: Run browser tests in parallel
    command: |
      . venv/bin/activate
      source ./env_vars/.env_with_export
      export PATH=$PATH:$HOME/bin
      if [ $(( ${CIRCLE_NODE_INDEX} % 2 )) = 0 ];
      then
          export BROWSER="chrome";
          google-chrome --version;
      else
          export BROWSER="firefox";
          firefox --version;
          export TAGS="${TAGS} --tags=~@skip-in-firefox"
      fi

      cd tests/browser
      echo "Found `wc -l < scenario_titles.txt` scenarios to run"
      # cat scenario_titles.txt | circleci tests split > /tmp/tests-to-run
      # calculate number of lines after which list of scenarios will be split
      SPLIT_AFTER_LINES=$(( $(wc -l < scenario_titles.txt) / $((${CIRCLE_NODE_TOTAL} / 2)) ))
      # split list of scenarios into equal parts (equal to half on number of nodes)
      # as half of nodes run tests in Chrome & the other half in Firefox
      split -d --suffix-length=1 --lines ${SPLIT_AFTER_LINES} scenario_titles.txt tests-to-run

      SPLIT_FILE_SUFFIX=$(( ${CIRCLE_NODE_INDEX} / 2 ))

      echo "Current instance of ${BROWSER} is going to run `cat tests-to-run${SPLIT_FILE_SUFFIX} | wc -l` scenarios:"
      cat tests-to-run${SPLIT_FILE_SUFFIX}


      if [ -d "/home/circleci/great_magna_tests/tests/browser/reports" ]; then rm -Rf "/home/circleci/great_magna_tests/tests/browser/reports"; fi

      mkdir /home/circleci/great_magna_tests/tests/browser/reports

      cat tests-to-run${SPLIT_FILE_SUFFIX} | while IFS=$'\r' read -r title ; do
        echo -e "\n\n${BROWSER} run:    ${title}"
        PYTHONPATH=. \
        LOG_FILE="reports/behave_${BROWSER}_${CIRCLE_NODE_INDEX}.log" \
        timeout --verbose --preserve-status --kill-after=5s 5m \
        behave features/ \
        --format=allure_behave.formatter:AllureFormatter \
        --define AllureFormatter.issue_pattern=${BUG_TRACKER_URL_PATTERN} \
        --define AllureFormatter.link_pattern=${BUG_TRACKER_URL_PATTERN} \
        --outfile=results_${BROWSER}_${CIRCLE_NODE_INDEX}/ \
        --no-skipped \
        --no-source \
        --no-summary \
        --junit \
        --junit-=./junit_reports_${BROWSER}_${CIRCLE_NODE_INDEX}/ \
        --tags=~@wip \
        --tags=~@fixme \
        --tags=~@skip \
        --tags=~@eu-exit \
        ${TAGS} \
        --name "${title}" && {
          echo "${BROWSER} passed: ${title}"
        } || {
          echo "${BROWSER} failed: ${title}"
        }
      done
      echo "Found `ls -la results_${BROWSER}_${CIRCLE_NODE_INDEX}/*.json | wc -l` JSON Allure result files in results_${BROWSER}_${CIRCLE_NODE_INDEX}/"
      echo "Found `ls -la junit_reports_${BROWSER}_${CIRCLE_NODE_INDEX}/*.xml | wc -l` JUnit XML result files in junit_reports_${BROWSER}_${CIRCLE_NODE_INDEX}/"

      echo "Saving environment variables that control test execution"
      ../../save_test_related_env_vars.py


step_run_load_tests: &step_run_load_tests
  run:
    name: Run load tests
    command: |
      . venv/bin/activate
      python ./env_vars/env_writer.py --env=${TEST_ENV} --config=./env_vars/env.json
      source ./env_vars/.env_with_export;
      NUM_USERS=${NUM_USERS} HATCH_RATE=${HATCH_RATE} RUN_TIME=${RUN_TIME} make load_test_${SERVICE}

step_run_geckoboard_updater: &step_run_geckoboard_updater
  run:
    name: Run Geckoboard Updater script
    command: |
      . venv/bin/activate
      make geckoboard_updater

# this step requires Java 8+ to work.
step_merge_junit_xml_reports_from_parallel_browser_tests: &step_merge_junit_xml_reports_from_parallel_browser_tests
  run:
    name: Merge JUnit XML reports & print errors summary
    when: always
    command: |
      python3 -m venv venv
      . venv/bin/activate
      pip install --quiet --upgrade pip
      cd ./tests/browser/
      mkdir junit_report
      mv junit_reports_* junit_report/
      ls -la junit_report/
      junit-merge --recursive --dir junit_report/ --out ./junit_report/merged.xml
      rm -fr junit_report/junit_reports_*
      cd ../../
      ./print_error_summary.py --report ./tests/browser/junit_report/merged.xml

steps:
  - *step_checkout_repo

steps_setup_env_for_parallel_browser_tests_using_circleci: &steps_setup_env_for_parallel_browser_tests_using_circleci
  steps:
    - *step_checkout_repo
    - *step_install_requirements_for_parallel_browser_tests
    - *step_cache_browser_tests_requirements

steps_setup_env_for_load_tests: &steps_setup_env_for_load_tests
  steps:
    - *step_checkout_repo
    - *step_install_requirements_for_load_tests
    - *step_cache_load_tests_requirements

steps_merge_browser_allure_results_and_generate_report_and_send_link_to_slack: &steps_merge_browser_allure_results_and_generate_report_and_send_link_to_slack
  steps:
    - checkout
    - attach_workspace:
        at: ./
    - *step_install_moxci
    - *step_install_allure_cli
    - *step_update_allure_result_files_generated_by_browser_tests
    - *step_make_allure_report
    - store_artifacts:
        path: ./allure_report/
        destination: allure_report
    - run:
        name: Push Allure report link to Slack with moxci
        when: always
        command: |
          set -e
          export CIRCLE_PULL_REQUEST="${CIRCLE_PULL_REQUEST:-none}"
          export WORKFLOW_LINK="https://circleci.com/workflow-run/${CIRCLE_WORKFLOW_ID}"
          export summary="$(./parse_test_summary_json.py allure_report/widgets/summary.json)"
          moxci allure_report/index.html --slack_message "The latest report from BROWSER tests ran against `awk '/Environment/{print toupper($3)}' ./results/environment.properties`. ${summary}. (workflow → ${WORKFLOW_LINK})"

steps_run_parallel_browser_tests: &steps_run_parallel_browser_tests
  steps:
    - *step_restore_cached_browser_tests_requirements
    - *step_prepare_list_of_scenarios_to_run_by_parallel_browser_tests
    - *step_run_browser_tests_in_parallel
    - persist_to_workspace:
        root: .
        paths:
          - .

steps_run_load_tests: &steps_run_load_tests
  steps:
    - *step_restore_cached_load_tests_requirements
    - *step_run_load_tests
    - store_artifacts:
        path: ./reports

step_restore_cached_periodic_tasks_requirements: &step_restore_cached_periodic_tasks_requirements
  restore_cache:
    key: periodic-tasks-dependency-cache-{{ .Revision }}

steps_geckoboard_updater: &steps_geckoboard_updater
  steps:
    - *step_restore_cached_periodic_tasks_requirements
    - *step_run_geckoboard_updater

steps_setup_env_for_periodic_tasks: &steps_setup_env_for_periodic_tasks
  steps:
    - *step_checkout_repo
    - *step_install_requirements_for_periodic_tasks
    - *step_cache_periodic_tasks_requirements

setup_env_for_periodic_tasks: &setup_env_for_periodic_tasks
  working_directory: ~/great_magna_tests
  <<: [
  *machine_python3_clean,
  *steps_setup_env_for_periodic_tasks,
  ]

jobs:

  setup_env_for_load_tests:
    working_directory: ~/great_magna_tests
    <<: [
      *machine_python3_node,
      *steps_setup_env_for_load_tests,
    ]

  setup_env_for_parallel_browser_tests_using_circleci:
    working_directory: ~/great_magna_tests
    <<: [
      *machine_python3_node_browsers,
      *steps_setup_env_for_parallel_browser_tests_using_circleci,
    ]

  load_domestic_tests_stage:
    working_directory: ~/great_magna_tests
    environment:
      SERVICE: "product_search"
      TEST_ENV: "stage"
      <<: [
        *load_hatch_rate,
        *load_num_users,
        *load_run_time,
      ]
    <<: [
      *machine_python3_node,
      *steps_run_load_tests,
    ]

  dev_parallel_browser_tests_using_circleci:
    working_directory: ~/great_magna_tests
    parallelism: 12
    environment:
      TEST_ENV: "dev"
      AUTO_RETRY: "true"
      TAKE_SCREENSHOTS: "true"
      TAGS: "--tags=~@stage-only --tags=~@uat-only --tags=~@erp"
      BROWSER_ENVIRONMENT: "local"
      BROWSER_HEADLESS: "true"
    <<: [
      *machine_python3_node_browsers,
      *steps_run_parallel_browser_tests,
    ]

  stage_parallel_browser_tests_using_circleci:
    working_directory: ~/great_magna_tests
    parallelism: 12
    environment:
      TEST_ENV: "stage"
      AUTO_RETRY: "true"
      TAKE_SCREENSHOTS: "true"
      TAGS: "--tags=~@dev-only --tags=~@uat-only --tags=~@erp"
      BROWSER_ENVIRONMENT: "local"
      BROWSER_HEADLESS: "true"
    <<: [
      *machine_python3_node_browsers,
      *steps_run_parallel_browser_tests,
    ]

  uat_parallel_browser_tests_using_circleci:
    working_directory: ~/great_magna_tests
    parallelism: 12
    environment:
      TEST_ENV: "uat"
      AUTO_RETRY: "true"
      TAKE_SCREENSHOTS: "true"
      TAGS: "--tags=~@stage-only --tags=~@erp"
      BROWSER_ENVIRONMENT: "local"
      BROWSER_HEADLESS: "true"
    <<: [
      *machine_python3_node_browsers,
      *steps_run_parallel_browser_tests,
    ]

  merge_browser_allure_results_and_generate_report_and_send_link_to_slack:
    working_directory: ~/great_magna_tests
    <<: [
      *machine_python3_node_browsers,
      *steps_merge_browser_allure_results_and_generate_report_and_send_link_to_slack,
    ]

  merge_junit_results_from_parallel_browser_tests:
    docker:
      - image: circleci/python:3.8.0-node-browsers
    working_directory: ~/great_magna_tests
    steps:
      - *step_install_junit_merge
      - checkout
      - attach_workspace:
          at: ./
      - *step_merge_junit_xml_reports_from_parallel_browser_tests
      - store_test_results:
          path: ./tests/browser/junit_report/
      - store_artifacts:
          path: ./tests/browser/junit_report/
          destination: junit_report

  store_behave_logs_from_browser_tests_as_artifacts:
    <<: *machine_python3_clean
    working_directory: ~/great_magna_tests
    steps:
      - attach_workspace:
          at: ./
      - store_artifacts:
          path: ./tests/browser/reports/
          destination: behave_logs

  refresh_geckoboard:
    working_directory: ~/great_magna_tests
    <<: [
    *machine_python3_clean,
    *steps_geckoboard_updater,
    ]

workflows:
  version: 2

  ####################################################################
  ## Ad hoc workflows - these are run every time you push a change
  ####################################################################

  run_browser_tests_in_parallel_in_dev_ad_hoc:
    jobs:
      - setup_env_for_parallel_browser_tests_using_circleci
      - dev_parallel_browser_tests_using_circleci:
          requires:
            - setup_env_for_parallel_browser_tests_using_circleci
      - merge_browser_allure_results_and_generate_report_and_send_link_to_slack:
          requires:
            - dev_parallel_browser_tests_using_circleci
      - merge_junit_results_from_parallel_browser_tests:
          requires:
            - dev_parallel_browser_tests_using_circleci
      - store_behave_logs_from_browser_tests_as_artifacts:
          requires:
            - dev_parallel_browser_tests_using_circleci

  run_browser_tests_in_parallel_in_stage_ad_hoc:
    jobs:
      - setup_env_for_parallel_browser_tests_using_circleci
      - stage_parallel_browser_tests_using_circleci:
          requires:
            - setup_env_for_parallel_browser_tests_using_circleci
      - merge_browser_allure_results_and_generate_report_and_send_link_to_slack:
          requires:
            - stage_parallel_browser_tests_using_circleci
      - merge_junit_results_from_parallel_browser_tests:
          requires:
            - stage_parallel_browser_tests_using_circleci
      - store_behave_logs_from_browser_tests_as_artifacts:
          requires:
            - stage_parallel_browser_tests_using_circleci

  run_browser_tests_in_parallel_in_uat_ad_hoc:
    jobs:
      - setup_env_for_parallel_browser_tests_using_circleci
      - uat_parallel_browser_tests_using_circleci:
          requires:
            - setup_env_for_parallel_browser_tests_using_circleci
      - merge_browser_allure_results_and_generate_report_and_send_link_to_slack:
          requires:
            - uat_parallel_browser_tests_using_circleci
      - merge_junit_results_from_parallel_browser_tests:
          requires:
            - uat_parallel_browser_tests_using_circleci
      - store_behave_logs_from_browser_tests_as_artifacts:
          requires:
            - uat_parallel_browser_tests_using_circleci

  refresh_geckoboard_ad_hoc:
    jobs:
      - setup_env_for_periodic_tasks
      - refresh_geckoboard:
          requires:
            - setup_env_for_periodic_tasks

  ####################################################################
  ## Scheduled workflows
  ####################################################################

  run_browser_tests_in_parallel_in_dev:
    triggers:
      - schedule:
          cron: "39 1 * * 1-5"
          filters:
            branches:
              only: master
    jobs:
      - setup_env_for_parallel_browser_tests_using_circleci
      - dev_parallel_browser_tests_using_circleci:
          requires:
            - setup_env_for_parallel_browser_tests_using_circleci
      - merge_browser_allure_results_and_generate_report_and_send_link_to_slack:
          requires:
            - dev_parallel_browser_tests_using_circleci
      - merge_junit_results_from_parallel_browser_tests:
          requires:
            - dev_parallel_browser_tests_using_circleci
      - store_behave_logs_from_browser_tests_as_artifacts:
          requires:
            - dev_parallel_browser_tests_using_circleci

  run_browser_tests_in_parallel_in_stage:
    triggers:
      - schedule:
          cron: "23 3 * * 1-5"
          filters:
            branches:
              only: master
    jobs:
      - setup_env_for_parallel_browser_tests_using_circleci
      - stage_parallel_browser_tests_using_circleci:
          requires:
            - setup_env_for_parallel_browser_tests_using_circleci
      - merge_browser_allure_results_and_generate_report_and_send_link_to_slack:
          requires:
            - stage_parallel_browser_tests_using_circleci
      - merge_junit_results_from_parallel_browser_tests:
          requires:
            - stage_parallel_browser_tests_using_circleci
      - store_behave_logs_from_browser_tests_as_artifacts:
          requires:
            - stage_parallel_browser_tests_using_circleci


  run_browser_tests_in_parallel_in_uat:
    triggers:
      - schedule:
          cron: "3 6 * * 1-5"
          filters:
            branches:
              only: master
    jobs:
      - setup_env_for_parallel_browser_tests_using_circleci
      - uat_parallel_browser_tests_using_circleci:
          requires:
            - setup_env_for_parallel_browser_tests_using_circleci
      - merge_browser_allure_results_and_generate_report_and_send_link_to_slack:
          requires:
            - uat_parallel_browser_tests_using_circleci
      - merge_junit_results_from_parallel_browser_tests:
          requires:
            - uat_parallel_browser_tests_using_circleci
      - store_behave_logs_from_browser_tests_as_artifacts:
          requires:
            - uat_parallel_browser_tests_using_circleci


  refresh_geckoboard_periodically:
    triggers:
      - schedule:
          cron: "0 8,13 * * 1-5"
          filters:
            branches:
              only: master
    jobs:
      - refresh_geckoboard